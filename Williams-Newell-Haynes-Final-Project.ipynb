{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Game Playing Algorithms with Checkers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ryan Williams, Ben Newell, Chris Haynes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we decided to use different AI game playing algorithms on checkers to see which ones play the best. For the rules of checkers, visit [this website](https://www.itsyourturn.com/t_helptopic2030.html). We chose the following four game playing algorithms: minimax, negamax, reinforcement learning with temporal difference and reinforcement learning with a neural network. Additionally, we created a board python class to represent the board for the game handling all of the game playing mechanics like picking a valid move, making a move, determining if the game is over, etc. After implementation, we proceeded to run the algorithms against opponents who choose random moves and then the other game playing algorithms to see which one would come out on top.\n",
    "\n",
    "The dividing of the workload was done as follows:\n",
    "* Ryan Williams - Minimax and Negamax\n",
    "* Ben Newell - Reinforcement learning with a neural network\n",
    "* Chris Haynes - Board class, Piece class, and reinforcement learning with temporal difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Imports and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dill'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-e244b9f9bf20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mreinforcement\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscaledconjugategradient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\CSU_SENIOR_Fall\\CS_440\\Final_Project\\clone1\\gneurally_checkers\\test.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnegamax\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnnutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mthreading\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\CSU_SENIOR_Fall\\CS_440\\Final_Project\\clone1\\gneurally_checkers\\nnutils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdill\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpickle\u001b[0m \u001b[1;31m# this library is like regular pickle but tastier. Serializes lambdas.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mboard\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dill'"
     ]
    }
   ],
   "source": [
    "inf = float('infinity')\n",
    "from board import Board\n",
    "from piece import Color, Piece\n",
    "from negamax import negamax\n",
    "from minimax import minimax\n",
    "import neuralnetworks as nr\n",
    "from NeuralReinforcement import *\n",
    "from reinforcement import *\n",
    "from scaledconjugategradient import *\n",
    "from test import *\n",
    "from copy import copy, deepcopy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Board Class\n",
    "### Overview\n",
    "This class contains the entire functionality of checkers. It handles everything required to turn the game of checkers into a trainable game for the different AI algorithms. The functions that we have learned throughout the semester that are quintessential to many algorithms, such as `stateMoveTuple`, `validMoves`, etc. all reside in the board class. Since the class is very large, the functions will be described in the order they appear to make it easier to understand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructor: `__init__(self)`\n",
    "This function initializes the game of checkers. It uses an 8x8 numpy array called `draught` (the official name of the checkers board) of `Piece` objects to initialize the board. If a piece is absent, the space is set to `None`. Initial positions are the first three and last three rows and alternate between a left alignment and right alignment. Since the color red starts the game, the turn is set to `Color.RED`, where color is an enumeration. This could have been simplified to a boolean saying `red_turn = True` for red's turn and `red_turn = False` for black's turn. However for readability of my teammates I added the enumeration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representation: `__repr__(self)`\n",
    "This function converts a board to a string so it is easier to print and make `stateMoveTuples`. It creates a format string that represents a row, and for every row add the `Piece` string representation and if it is `None` add a dash to represent the empty space. It has indexed columns and rows that correspond to the indices of the numpy array `draught`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print State: `printState(self)`\n",
    "This function prints a board, which calls the `__repr__` function described earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State Move Tuple: `stateMoveTuple(self, move)`\n",
    "This function takes a move as input and returns a tuple with the state of the board as a string and a move (which is already a tuple)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Move: `makeMove(self, move)`\n",
    "This function takes a move and will make a move on the board. It uses the length of the move tuple to determine if any jumps occur during the move. If a jump is made, the pieces that were jumped during the move are set to `None` in the `draught`. It will then set the pieces previous space to `None` and update the space that it is moving to with moved `Piece`. If a `Piece` has reached the opponents edge of the `draught`, the `Piece` will be promoted to a king. The turn is then changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Turn: `changeTurn(self)`\n",
    "This changes the turn of the board using a ternary operator to determine which turn to set the `Board` to the correct turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valid Moves: `validMoves(self)`\n",
    "This function determines the list of valid moves for a `Board` based on the turn. It begins by filtering for the `valid_pieces` on the `Board`. An array is initialized to hold all `valid_moves`. For every piece in the `valid_pieces` it checks if it can move left or right based on its color if it is not a king. So black can move up in rows and red can move down. Kings can move in all directions. This uses the `checkSpace` helper function to determine legal moves in that direction. It filters out the moves returned and adds it to the array it returns in the tuple format of `(initial position, move)`. For multiple jumps, they are recorded in the move. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Space: `checkSpace(self, position, row_mod, col_mod, king=False, recurse=False, positions=None)`\n",
    "This function takes as input an initial position, directions to move, whether or not the piece is a king, and recursion arguments. It starts by getting the `new_row` and `new_col` for a space to check. If it is in bounds, a check is performed to see if it is unoccupied. If it is, and it's not recursing (which happens during a jump), it will return that position only since it is an unoccupied space. If it is occupied and can be jumped and hasn't already been added to a jump pattern, it will check for extra jumps by checking the left and right positions again recursively. For kings it checks three positions instead of four (in `validMoves` it uses four) because a piece may not be jumped twice. If more jumps are found, they are added to the move. Return the list of valid moves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can Be Jumped: `canBeJumped(self, attacker, defender)`\n",
    "Checks if a piece can be jumped. It takes as input an attacking position and a defending position. The massive if statement determines the direction of the attack and if the piece has no piece on the other side of a jump, it may be jumped and will return `True`. In the case that it cannot be jumped, it will return `False`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piece Class\n",
    "## Overview\n",
    "This class represents a `Piece` on the `Board` in the `Board.draught` array. The class is fairly simple, but contains data critical to a `Piece`'s functionality in the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructor: `__init__(self, color, initial_position)`\n",
    "The constructor take as input the `Color` of the `Piece`, and it's `initial_position` on the board, which is a tuple in the form of (row, column). It is initialized as a normal piece rather than a king, so `king = False`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representation: `__repr__(self)`\n",
    "This is the string function for a `Piece`. It is the `Color` of the piece as a character ('B' for `Color.BLACK` and 'R' for `Color.RED`. If it is a king, a 'K' is appended to the string. So a red king is represented as 'RK' instead of just 'R'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### King Me: `king_me(self)`\n",
    "Promotes a `Piece` to king by setting `Piece.king` to `True`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement \n",
    "## Overview\n",
    "Notice that this is not a class like `Board` and `Piece` but rather just functions. These functions will both train and use a `Q` dictionary that stores the a `Board` state move tuple as the key to a number that represents the potential game outcome of making that particular move for that exact scenario. The higher the number, the better that move is for the AI. Our reinforcement is a little bit different than the reinforcement learning we saw in class. This algorithm back propagates a reinforcement throughout every move that led to that state, so that moves early in the game still recieve a value. Also, we significantly reduce the size of the `Q` table by storing just moves, rather than a state move tuple. Checkers has 500 billion billion possible states, not including state move tuples. We aslo did this because moves hold interesting information about the game, such as jumping another piece, getting kinged, etc. due to the way they are represented in the board class, and so that AI can extrapolate the usefulness of the move based on the outcome of the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon Greedy: `epsilonGreedy(epsilon, Q, board)`\n",
    "This function selects a move for a board based on an epsilon to add randomness into the selection. It takes an `epsilon` and will choose either a random move if a randomly generated number is less than the `epsilon`. The process is that a list of valid moves is generated for a `Board`. If the random route is selected, the a random move from `valid_moves` is selected. Otherwise, the greedy move for the `Q` dictionary is selected and returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy: `greedy(valid_moves, Q, board)`\n",
    "This function will make the greedy move for a board, or basically the move with the most utility as determined by the `Q` dictionary. For every move in `valid_moves`, if the state move tuple is in the dictionary, get its determined utility. Otherwise, it will be 0, a neautral utility because its usefulness has not yet been determined. It will then return the move with the highest utility. If all moves have the same value (most likely zero at the beginning), the first valid move will be chosen. For red this is the first move for the piece with the lowest row first, and lowest column second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finished: `finished(board)`\n",
    "Determines if a game is finished. If the length of valid moves from the `board` is zero, the game is over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Q: `trainQ(nRepititions, learningRate, epsilonDecayFactor, propagationDecayFactor)`\n",
    "This function trains a `Q` dictionary for the number `nRepititions` games. The `learningRate` determines how fast the `Q` dictionary will learn for a certain move and board state. The `epsilonDecayFactor` determines the rate at which the `epsilon` passed to `epsilonGreedy` will decay, meaning using the utilities stored in the `Q` more and more. Every game is a new game, in which the AI is set to play as red. For every game, if the game is finished, update the dictionary with a reinforcement. If it is a win, that move will be represented by a 1. If it is a loss it will be -2. We selected -2 since it is playing a random opponent, and losing to a random move maker should be punished harsher than winning against that random player. After a move has been made, it will back propagate that move's utility throughout every move made that led up to it by calling `backPropagateReinforcement`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back Propagate Reinforcement: `backPropagateReinforcement(Q, move_tuple_list, learningRate, propagationDecayFactor)`\n",
    "This function will back propagate a reinforcement through the Q dictionary. The reason we added this is there were too many zeros in the Q dictionary, and thus only first moves were selected a decent amount of the time as a result of np.argmax returning the first valid move when a move doesn't have a reinforcement. It back propagates a reinforcement throughout every move made that led to the final board state, so that every move has a reinforcement (every move matters!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Q: `useQ(Q, maxSteps)`\n",
    "This function tests the `Q` dictionary that results from the `trainQ` function. It uses the same game logic as `trainQ`. Red starts the game (red is the reinforcement trained AI). It makes a greedy move by calling `greedy` to determine the optimal move. Then black makes a random move and the game continues until one has lost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "There were some interesting observations along the way that led to structuring our reinforcement AI the way we did. First of all, we saw that using state move tuples worked just fine and the AI would beat the random with state move tuples as the keys to the `Q` dictionary. However, upon inspection there were way too many states and `Q` was very sparse after training, with often only values for less than 10% of all entries. \n",
    "\n",
    "The first method we tried to solve this was to back propagate the reinforcement with a very low decay rate so that the reinforcement would reach the opening move. However, these states were never used because there were too many to ever get a hit on the table. So how was that AI winning? Well, `numpy.argmax` in the `greedy` function takes the first valid move, so the strategy was inadvertantly to move the first piece possible up and to the left if it could, and never to move the last row. Since black could never move into the last row and make a king, it runs out of valid moves and the AI would win. \n",
    "\n",
    "To try and reduce this strategy and allow `Q` to actually contain some semblance of a strategy, we reduced `Q` to only contain the move selected. The move has inherent state involved, such as where it is moving from, the spaces it will jump, and the end position. This seemed to be enough to allow the AI to still ~98-99% of the time against a random opponent. This way we could look at the Q dictionary and see the values for the moves it selected. It still used the strategy of avoiding moving out of the back row if it could, but at times would jump a piece that had gotten close. It also really valued moving up the board as much as possible to try and get a king. If these moves were available they were often chosen, which could be a residual impact of the `numpy.argmax` usage we discussed earlier. Finally, it's winning strategy after observing many games. It tries it's best to get as many kings as possible, and leaves it's back row completely full, thus blocking the opponent from making any kings. Once it has kings and black has no kings, the opponent will inevitably run out of moves, and the AI will win the game.\n",
    "\n",
    "#### Further Possible Research\n",
    "Some interesting thoughts we had along the way and after discussing was using a smart test subject to train against. When looking at the moves made, the AI will take risks because a random opponent will rarely select the right move that would punish that risk, such as moving up the board and being vulnerable for a jump. It has one thing in mind: getting a king. To try and prevent foolish behavior such as this, a smarter training opponent could be added that would punish the AI by jumping it when it could. \n",
    "\n",
    "Another interesting idea is to use a neural net to determine the reinforcements. Possible reinforcements could be jumping pieces, getting kings, favoring the sides as opposed to the middle, and not moving off the back wall. If weights were determined through a neaural net for these reinforcements, the AI could be much smarter, and produce more accurate reinforcements for certain moves. \n",
    "\n",
    "Finally, we thought that it would be interesting to represent the keys to the `Q` dictionary in different ways. One could include a localized board section, where selecting the locally optimum would hopefully result in the global optimum. Another could be recognizing similar board patterns and using those patterns with move types (normal, jump, king, etc) as inputs into the dictionary. The point of changing how keys are represented is to reduce the number of states, while still maintaining a decently accurate `Q` dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkers is a zero-sum game, meaning the payoff for each player at the end of the game is equal and opposite. For this reason, adversarial search algorithms like `minimax` and `negamax` can be used to play the game of checkers. Through a process similar to a depth first search, the algorithms search through each possible move from the initial player. Then, from each of these moves, the algorithm searches each move from there for the opposing player. Once the depth limit or leaf nodes are reached, the utility is returned and the player chooses the move that optimizes their utility. From there, in the next recursion up, that player chooses it's optimum move and so on. This process results in one of the key components of adversarial search algorithms, each player assumming the other is playing optimally. Both of the two adversarial search algorithms, `minimax` and `negamax` follow this same process. The way they differ is in how players' utility is determined, which will be explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary funtions added to Board class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### isOver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`isOver` determines if the game is over, and if it is, gives the winner. It does this by checking the valid moves for each player. If the player has no valid moves, they have lost and the other player is the winner. This can be done by taking all of the opposing pieces (no pieces mean no valid moves) or by trapping all of the opponents pieces so they cannot move. If both players have valid moves they can do, the game is not over yet so the function returns `False` for the result and `None` for the winner.\n",
    "\n",
    "isOver(self)\n",
    "\n",
    "Parameters:\n",
    "* None\n",
    "\n",
    "Return:\n",
    "* boolean - true if game is over, false otherwise\n",
    "* winner - RED or BLACK depending on the winner. If game is not over, value is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getUtility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`getUtility` gets the utility for the calling player at the specific point in the game. If type is `nm` and the game is over, the function will return 2 if the calling player won and -2 if the calling player lost. If type is `mm`, however, the function will return 2 or -2 if the calling player won, 2 if it's the maximizer, -2 if it's the minimizer. If the calling player lost, -2 is returned if the caller is the maximizer and 2 is returned if the calling player is the minimizer.\n",
    "\n",
    "If the game is not over, then a heuristic is used to determine utility. The number of red and black pieces are counted with kings counting as two. If the type is `nm`, Whichever color has more pieces is awarded a utility of 1 and the color with less pieces receives a utility of -1 . Of course, if the type is `mm`, then the utilities are slightly different, with 1 going to maximizer if it has more pieces and -1 going to minimizer if it has more pieces. Therefore, the maximizer will get a utility of -1 if it has less pieces and the minimizer will get 1 if it has less. Finally, if the number of pieces is equal, a utility of 0 is returned.\n",
    "\n",
    "The utility function was made in this way so that winning supercedes all. At first, the thought was to use 1 as the utility for both winning and having more pieces, but we decided that it makes the most sense for the algorithm to choose the winning move if there is an option between that and merely having more pieces. The most simple way to do this was to set winning utility as greater than the heuristic so it would always be chosen.\n",
    "\n",
    "getUtility(self, type='nm', maximizePlayer=Color.RED)\n",
    "\n",
    "Parameters:\n",
    "* type: 'mm' or 'nm' depending on whether the algorithm is `minimax` or `negamax` since they give different utilities to each player\n",
    "* maximizePlayer: Color of the maximizing player\n",
    "\n",
    "Return:\n",
    "* integer - represents the utility given at this stage of the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two players in the minimax algorithm are `max` and `min`. `Max` will look to maximize the utility and `min` will look to minimize the utility. For our case, in our checkers implementation, a win for `max`is worth 2 and a win for `min` is worth -2. `Minimax` searches through every possible move in the game until the depth limit, assuming on the way that the opponent will play optimally. If the depth limit is reached, a simple heuristic is used to see who has the advantage. All of the pieces for each player will be counted, with kings counting as two. If `max` has more, a utility of 1 will be awarded; if `min` has more, however, a utility of -1 is awarded. \n",
    "\n",
    "One thing added to this algorithm to cut down run time was a `break` statement used if the returned utility is 2 for the maximizer or -2 for minimizer. This was done because these values signify a win for their respective player so there is no reason to search any other nodes on that level of the tree. Even though a win could be achieved with a different move yet to be searched from that state, the algorithm wouldn't even choose that path because the returned value would equal the `bestValue` but not be greater than it, so `bestValue` would not be updated anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alpha beta pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha beta pruning is a technique used in adversarial search algorithms to limit the number subtrees that need to be searched. It does this by taking the best value found so far for one player, represented by the variable `alpha`, and comparing it to the best value found so far for the other player, represented by the variable `beta`. If `alpha` is ever greater than `beta`, all other subtrees at that level do not need to be searched and can be pruned because, even if they were to be searched, they would not be chosen by the algorithm. To explain further, even though the current node may have chosen a value in the pruned subtree, the node above the current is guarenteed to not choose anything in that subtree since it has already found a better value in a previous subtree. As a result, cutting off these subrees saves a lot of time.\n",
    "\n",
    "The implementation of alpha beta pruning is simple for `minimax`. First, The parameters `alpha` and `beta` are added to the funtion with `alpha` always representing the best value for the maximizer and `beta` always representing the best value for the minimizer. Inside the function, if the current player is the maximizer, after a value and a move is returned by the recursive call to `minimax`, the current best value found is compared to `alpha`. If `bestValue` is greater than `alpha`, `alpha` will be set to `bestValue`. If the current player is the minimzer, however, the current best value is compared to `beta` and if `bestValue` is less than `beta`, `beta` is set to `bestValue`. Finally, following this process for both the maximizer and minimizer, `alpha` is compared to `beta` and if `alpha` is greater than `beta`, a `break` statement is used and no more of the moves for that specific state are checked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minimax(board, depthLeft, alpha, beta, isMax=True)\n",
    "\n",
    "Parameters:\n",
    "* `board` : board object with necessary functions like validMoves, makeMove, etc.\n",
    "* `depthLeft` : depth left before depth limit is reached and heuristic is used.\n",
    "* `alpah` : alpha value for alpha beta pruning, represents best value so far for the maximizing player\n",
    "* `beta` : beta value for alpha beta pruning, represents best value so far for the minimizing player\n",
    "* `isMax` : deliniates whether this player is the maximizing player. \n",
    "\n",
    "Return:\n",
    "* best value found and the resulting best move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negamax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As opposed to `minimax`, both players are looking to maximize their utility. As a result, a win for either player yields a utility of 2 and if the depth limit is reached, the heuristic is used giving either player a utility of one if they have more pieces than the other player. The algorithm flips the sign of the returned value from the recursive call, allowing both sides to look to maximize utility. Like the `minimax` algorithm, `negamax` also assumes the opponent will play optimally. \n",
    "\n",
    "Similar to `minimax`, a `break` statement is used if the utility is ever two, representing a win. The logic behind this operation is the same as the logic in `minimax`. If a win is found there is no reason to search any other moves because nothing better will be found, and, based on the way the algorithm updates the `bestValue`, once a win is discovered, another win will not result in `bestValue` being update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alpha beta pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha beta pruning works very similarly in `negamax` to the way it does in `minimax`. `alpha` and `beta` are still added to the function definition as parameters, but one difference is `alpha` is the best value so far for the current player and `beta` is the best value so far for the opposing player. Additionally, in the recursive call to the negamax function, `-beta` is passed in as the `alpha` parameter and `-alpha` is passed in as the `beta` parameter. This is done so, inside the recursive call, the `alpha` and `beta` values correctly represent the best so far values for their respective players. Despite the differences, the function of alpha beta pruning remains the same for `negamax` as it is for `minimax`. If `alpha` is ever greater than `beta`, all of the subtrees at that level can be pruned because they would never be chosen by the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "negamax(board, depthLeft, alpha, beta)\n",
    "\n",
    "Parameters:\n",
    "* `board` : board object with necessary functions like validMoves, makeMove, etc.\n",
    "* `depthLeft` : depth left before depth limit is reached and heuristic is used.\n",
    "* `alpah` : alpha value for alpha beta pruning, represents best value so far for the maximizing player\n",
    "* `beta` : beta value for alpha beta pruning, represents best value so far for the minimizing player\n",
    "\n",
    "Return:\n",
    "* best value found and the resulting best move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, test with a game one move away from being over "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0 |  1 |  2 |  3 |  4 |  5 |  6 |  7\n",
      " 0 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 1 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 2 | -  | B  | -  | -  | -  | -  | -  | - \n",
      " 3 | -  | -  | R  | -  | -  | -  | -  | - \n",
      " 4 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 5 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 6 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 7 | -  | -  | -  | -  | -  | -  | -  | - \n",
      "\n",
      "Value:  2 \n",
      "\n",
      "      0 |  1 |  2 |  3 |  4 |  5 |  6 |  7\n",
      " 0 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 1 | R  | -  | -  | -  | -  | -  | -  | - \n",
      " 2 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 3 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 4 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 5 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 6 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 7 | -  | -  | -  | -  | -  | -  | -  | - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def minimaxExample1():\n",
    "    # Setup\n",
    "    board = Board()\n",
    "    piece = Piece(Color.RED, [3, 2])\n",
    "    piece2 = Piece(Color.BLACK, [2, 1])\n",
    "    draught = np.empty(shape=(8, 8), dtype=object)\n",
    "    draught[3, 2] = piece\n",
    "    draught[2, 1] = piece2\n",
    "    board.setBoard(draught)\n",
    "    board.printState()\n",
    "\n",
    "    value, move = minimax(board, 9, -inf, inf)\n",
    "    board.makeMove(move)\n",
    "    print(\"Value: \", value, \"\\n\")\n",
    "    board.printState()\n",
    "    \n",
    "minimaxExample1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, test with a game a few moves away from being over. As you can see, minimax chooses the moves that lead to the fastest win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0 |  1 |  2 |  3 |  4 |  5 |  6 |  7\n",
      " 0 | -  | -  | -  | B  | -  | -  | -  | - \n",
      " 1 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 2 | -  | -  | -  | B  | -  | R  | -  | - \n",
      " 3 | -  | -  | R  | -  | R  | -  | -  | - \n",
      " 4 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 5 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 6 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 7 | -  | -  | -  | -  | -  | -  | -  | - \n",
      "\n",
      "move:  ([3, 4], ((1, 2),))\n",
      "\n",
      "Player Color.RED to ([3, 4], ((1, 2),)) for value 2\n",
      "      0 |  1 |  2 |  3 |  4 |  5 |  6 |  7\n",
      " 0 | -  | -  | -  | B  | -  | -  | -  | - \n",
      " 1 | -  | -  | R  | -  | -  | -  | -  | - \n",
      " 2 | -  | -  | -  | -  | -  | R  | -  | - \n",
      " 3 | -  | -  | R  | -  | -  | -  | -  | - \n",
      " 4 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 5 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 6 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 7 | -  | -  | -  | -  | -  | -  | -  | - \n",
      "\n",
      "move:  ([0, 3], ((2, 1),))\n",
      "\n",
      "Player Color.BLACK to ([0, 3], ((2, 1),)) for value 2\n",
      "      0 |  1 |  2 |  3 |  4 |  5 |  6 |  7\n",
      " 0 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 1 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 2 | -  | B  | -  | -  | -  | R  | -  | - \n",
      " 3 | -  | -  | R  | -  | -  | -  | -  | - \n",
      " 4 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 5 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 6 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 7 | -  | -  | -  | -  | -  | -  | -  | - \n",
      "\n",
      "move:  ([3, 2], ((1, 0),))\n",
      "\n",
      "Player Color.RED to ([3, 2], ((1, 0),)) for value 2\n",
      "      0 |  1 |  2 |  3 |  4 |  5 |  6 |  7\n",
      " 0 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 1 | R  | -  | -  | -  | -  | -  | -  | - \n",
      " 2 | -  | -  | -  | -  | -  | R  | -  | - \n",
      " 3 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 4 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 5 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 6 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 7 | -  | -  | -  | -  | -  | -  | -  | - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def minimaxExample2():\n",
    "    board = Board()\n",
    "    piece = Piece(Color.RED, [3, 4])\n",
    "    piece4 = Piece(Color.RED, [3, 2])\n",
    "    piece5 = Piece(Color.RED, [2, 5])\n",
    "    piece2 = Piece(Color.BLACK, [2, 3])\n",
    "    piece3 = Piece(Color.BLACK, [0, 3])\n",
    "    draught = np.empty(shape=(8, 8), dtype=object)\n",
    "    draught[3, 4] = piece\n",
    "    draught[3, 2] = piece4\n",
    "    draught[2, 5] = piece5\n",
    "    draught[2, 3] = piece2\n",
    "    draught[0, 3] = piece3\n",
    "    board.setBoard(draught)\n",
    "    board.printState()\n",
    "\n",
    "    isMax = True\n",
    "    isOver, _ = board.isOver()\n",
    "    while not isOver:\n",
    "        value, move = minimax(board, 10, -inf, inf, isMax)\n",
    "        print(\"move: \", move)\n",
    "        if move is None:\n",
    "            print('move is None. Stopping')\n",
    "            break\n",
    "        print(\"\\nPlayer\", board.turn, \"to\", move, \"for value\", value)\n",
    "        board.makeMove(move)\n",
    "        print(board)\n",
    "        isMax = not isMax\n",
    "        isOver, _ = board.isOver()\n",
    "        \n",
    "minimaxExample2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that running minimax against a random opponent takes a solid amount of time because so many game states have to be searched. Running this cell will probably take around 30 minutes to complete. Uncomment the last line in the cell to call this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimaxExample3():\n",
    "    board = Board()\n",
    "    board.printState()\n",
    "\n",
    "    isMax = True\n",
    "    isOver, _ = board.isOver()\n",
    "    while not isOver:\n",
    "        if board.turn == Color.BLACK:\n",
    "            move = board.validMoves()[int(len(board.validMoves()) / 2)]\n",
    "            print(\"\\nPlayer\", board.turn, \"to\", move)\n",
    "            board.makeMove(move)\n",
    "        else:\n",
    "            value, move = minimax(board, 10, -inf, inf, isMax)\n",
    "            print(\"move: \", move)\n",
    "            if move is None:\n",
    "                print('move is None. Stopping')\n",
    "                break\n",
    "            print(\"\\nPlayer\", board.turn, \"to\", move, \"for value\", value)\n",
    "            board.makeMove(move)\n",
    "        print(board)\n",
    "        isMax = not isMax\n",
    "        isOver, _ = board.isOver()\n",
    "        \n",
    "# minimaxExample3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negamax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with same scenarios as were tested with minimax\n",
    "\n",
    "Start with a game one move away from being over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0 |  1 |  2 |  3 |  4 |  5 |  6 |  7\n",
      " 0 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 1 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 2 | -  | B  | -  | -  | -  | -  | -  | - \n",
      " 3 | -  | -  | R  | -  | -  | -  | -  | - \n",
      " 4 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 5 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 6 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 7 | -  | -  | -  | -  | -  | -  | -  | - \n",
      "\n",
      "Value:  2 \n",
      "\n",
      "      0 |  1 |  2 |  3 |  4 |  5 |  6 |  7\n",
      " 0 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 1 | R  | -  | -  | -  | -  | -  | -  | - \n",
      " 2 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 3 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 4 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 5 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 6 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 7 | -  | -  | -  | -  | -  | -  | -  | - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def negamaxExample1():\n",
    "    # Setup\n",
    "    board = Board()\n",
    "    piece = Piece(Color.RED, [3, 2])\n",
    "    piece2 = Piece(Color.BLACK, [2, 1])\n",
    "    draught = np.empty(shape=(8, 8), dtype=object)\n",
    "    draught[3, 2] = piece\n",
    "    draught[2, 1] = piece2\n",
    "    board.setBoard(draught)\n",
    "    board.printState()\n",
    "\n",
    "    value, move = negamax(board, 9, -inf, inf)\n",
    "    board.makeMove(move)\n",
    "    print(\"Value: \", value, \"\\n\")\n",
    "    board.printState()\n",
    "    \n",
    "negamaxExample1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, test with a game a few moves away from being over. As you can see, negamax chooses the moves that lead to the fastest win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0 |  1 |  2 |  3 |  4 |  5 |  6 |  7\n",
      " 0 | -  | -  | -  | B  | -  | -  | -  | - \n",
      " 1 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 2 | -  | -  | -  | B  | -  | R  | -  | - \n",
      " 3 | -  | -  | R  | -  | R  | -  | -  | - \n",
      " 4 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 5 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 6 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 7 | -  | -  | -  | -  | -  | -  | -  | - \n",
      "\n",
      "move:  ([3, 4], ((1, 2),))\n",
      "\n",
      "Player Color.RED to ([3, 4], ((1, 2),)) for value 2\n",
      "      0 |  1 |  2 |  3 |  4 |  5 |  6 |  7\n",
      " 0 | -  | -  | -  | B  | -  | -  | -  | - \n",
      " 1 | -  | -  | R  | -  | -  | -  | -  | - \n",
      " 2 | -  | -  | -  | -  | -  | R  | -  | - \n",
      " 3 | -  | -  | R  | -  | -  | -  | -  | - \n",
      " 4 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 5 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 6 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 7 | -  | -  | -  | -  | -  | -  | -  | - \n",
      "\n",
      "move:  ([0, 3], ((2, 1),))\n",
      "\n",
      "Player Color.BLACK to ([0, 3], ((2, 1),)) for value -2\n",
      "      0 |  1 |  2 |  3 |  4 |  5 |  6 |  7\n",
      " 0 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 1 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 2 | -  | B  | -  | -  | -  | R  | -  | - \n",
      " 3 | -  | -  | R  | -  | -  | -  | -  | - \n",
      " 4 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 5 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 6 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 7 | -  | -  | -  | -  | -  | -  | -  | - \n",
      "\n",
      "move:  ([3, 2], ((1, 0),))\n",
      "\n",
      "Player Color.RED to ([3, 2], ((1, 0),)) for value 2\n",
      "      0 |  1 |  2 |  3 |  4 |  5 |  6 |  7\n",
      " 0 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 1 | R  | -  | -  | -  | -  | -  | -  | - \n",
      " 2 | -  | -  | -  | -  | -  | R  | -  | - \n",
      " 3 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 4 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 5 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 6 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 7 | -  | -  | -  | -  | -  | -  | -  | - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def negamaxExample2():\n",
    "    # Setup\n",
    "    board = Board()\n",
    "    piece = Piece(Color.RED, [3, 4])\n",
    "    piece4 = Piece(Color.RED, [3, 2])\n",
    "    piece5 = Piece(Color.RED, [2, 5])\n",
    "    piece2 = Piece(Color.BLACK, [2, 3])\n",
    "    piece3 = Piece(Color.BLACK, [0, 3])\n",
    "    draught = np.empty(shape=(8, 8), dtype=object)\n",
    "    draught[3, 4] = piece\n",
    "    draught[3, 2] = piece4\n",
    "    draught[2, 5] = piece5\n",
    "    draught[2, 3] = piece2\n",
    "    draught[0, 3] = piece3\n",
    "    board.setBoard(draught)\n",
    "    board.printState()\n",
    "\n",
    "    isOver, _ = board.isOver()\n",
    "    while not isOver:\n",
    "        value, move = negamax(board, 5, -inf, inf)\n",
    "        print(\"move: \", move)\n",
    "        if move is None:\n",
    "            print('move is None. Stopping')\n",
    "            break\n",
    "        print(\"\\nPlayer\", board.turn, \"to\", move, \"for value\", value)\n",
    "        board.makeMove(move)\n",
    "        print(board)\n",
    "        isOver, _ = board.isOver()\n",
    "        \n",
    "negamaxExample2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that running negamax against a random opponent takes a solid amount of time because so many game states have to be searched. Running this cell will probably take around 30 minutes to complete. Uncomment the last line in the cell to call this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negamaxExample3():\n",
    "    board = Board()\n",
    "    board.printState()\n",
    "\n",
    "    isOver, _ = board.isOver()\n",
    "    while not isOver:\n",
    "        if board.turn == Color.BLACK:\n",
    "            move = board.validMoves()[int(len(board.validMoves()) / 2)]\n",
    "            print(\"\\nPlayer\", board.turn, \"to\", move)\n",
    "            board.makeMove(move)\n",
    "        else:\n",
    "            value, move = negamax(board, 10, -inf, inf)\n",
    "            print(\"move: \", move)\n",
    "            if move is None:\n",
    "                print('move is None. Stopping')\n",
    "                break\n",
    "            print(\"\\nPlayer\", board.turn, \"to\", move, \"for value\", value)\n",
    "            board.makeMove(move)\n",
    "        print(board)\n",
    "        isOver, _ = board.isOver()\n",
    "        \n",
    "# negamaxExample3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations for Adversarial Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, time is a serious issue for these adversarial search algorithms. Without alpha beta pruning they run for an incredibly long time in the order of hours. As a result, alpha beta pruning is essentially a requirement for these algorithms as it cuts down the runtime immensely. However, even with the addition of alpha beta pruning, time is still a big issue for these adversarial search algorithms. Running an entire game against an opponent who chooses random moves still takes around 30 minutes, since most moves for each player need to be searched and there are 10<sup>20</sup>. \n",
    "\n",
    "Setting a depth limit and heuristic is also important for this problem since it saves a lot of time. However, the use of the depth limit could potentially lead to a move that isn't optimal being chosen, but the heuristic does its best in the attempt to choose what it sees as the best move once the depth limit is reached. Additionally, the depth limit must be chosen wisely. A lower depth limit will result in faster run time but a higher depth limit allows the algorithm to search deeper into the game, therefore giving it a higher likelyhood of choosing a good move. \n",
    "\n",
    "This leads to an issue encountered in the application of adversarial search to checkers. If the depth limit is low, when it is reached not enough has happened yet in the game for the heuristic to decide who has an advantage, and a utility of 0 is returned since both sides have an even amount of pieces. The utility being 0 happens for much of the first half of the game before a player is deemed to have an advantage, and, while bad moves are still avoided, the algorithm stll chooses the first of potentially many moves with a 0 utility possibly missing an optimum move.\n",
    "\n",
    "Another issue observed due to the heuristic was sometimes a player seemed to decide to capture another piece just to get the piece count to be equal or in their favor, even if that capture is not the best move to do. As described earlier, the first half of the game is spent with the utility being 0 since both sides have an equal piece count. One potential reason for this is a player choosing to capture a piece to achieve the 0 utility when it would have a -1 utility (1 if the player is the minimizer in minimax) by not capturing. When looking at the board at these times, the capture seems illogical, but is still chosen because of the piece count reason. This same phenomenon is observed closer to the end of the game when an advantage can be derived. Seemingly illogical captures still take place just so the player will still have a greater piece count. However, even though these moves seem illogical from the onset because they lead to immediate capture of the origional piece or merely result in a perceived disadvantage, maybe prioritzing capturing and piece count is a worthwile stategy to consider in a checkers game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Research and Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some further research that could be done for these adversarial search algorithms, most of which relating to speed. One of the biggest things that could be improved is creating a better heuristic. With a improved heuristic that can determine advantage earlier in the game with more levels of utilities, a wider range of values will be found earlier on. This would inevetably result in more pruning of subtrees since there will be more than just the five utility values of -2, -1, 0, 1, and 2 to compare. Additionally, the heuristic could be improved allowing for a prioritization of certain types of moves. For example, capturing or kinging could be prioritized if the utility would otherwise be the same. Even not moving certain pieces could be prioritized like not moving pieces in the back row. Statistics keeping on certain moves could also be done in an attempt to remember what moves are good, in a manner similar to reinforcement learning. This way, with knowledge of the statistics, certain moves could be chosen over others if the utility is otherwise the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction To Reinforcement Learning With Neural Networks\n",
    "This section of the notebook details the implementation of a checkers playing neural network using reinforcement learning.  Reinforcement learning teaches computers how to play certain games much like how you would teach an animal using classical conditioning. Say I wanted to teach my dog how to go fetch the newspaper. One strategy would be to let her into the front yard, and if she brings back the newspaper give her a big treat. Another might be to gradually teach her, by maybe throwing the newspaper as if it was a toying and giving her treats when she brings it back, or giving her treats when she runs up to the newspaper. Of course punishments for incorrect behavior could be used as well, but this doesnt work very well with my dog, who is a wild spirit. Reinforcement learning for computers works in the same way. Here, we are trying to learn the game of checkers. One strategy would be to give reinforcements along the way for each enemy piece captured, or for forming certain formations that are favored by expert checkers players. But these methods can lead to biases and might prevent the AI from learning new, novel ways of winning. So instead this AI is only given a reinforcement when it wins and a negative reinforcement when it doesnt. This is like only giving your dog a treat when they bring back the newspaper. This sparse reinforcement might make it hard to learn at first, but allows for more flexibility in terms of picking up strategies. In addition it simplifies the code greatly, because the only condition that is checked for to give reinforcements is a win or a loss. \n",
    "\n",
    "Previously, we implemented reinforcement learning using a Q dictionary. This means that each state in the game of checkers is saved, and reinforcements are doled out to each individual state. If you want to find out the value of a move, you simply find what it's Q value is in the dictionary entry for the relevant state and move pair. This implementation uses a neural network instead. See the definition of neural networks. Here we use the state and move pair as input to the neural network, and a Q value representing the value of the move as the output of the network. This has an advantage over the Q table approach because we can get output from the Q network for state,move pairs that havent been encountered before. The table has no way of knowing what sort of value to assign to new states. One drawback when compared to the Q table approach is that the network must train for much longer to obtain any sort of proficiency, and the sparseness of reinforcement makes it very difficult for the network to learn what states lead to wins or losses. This also makes the programming of the training more complicated. The strategy used here is to play a certain number of games in a row x and to save up the outcomes in a list. Then after that amount of games is played, feed them into the neural network to train it to recognize what moves led to a loss or a victory in that batch of games. This process is then repeated over y batches, the idea being that after each batch it would become better at the game and win more of the matches. (This didnt actually hold up to well, I think it has to do with either the number of iterations being too high and the variables going whack, or maybe an issue with my code, but I didnt find anything.)When training the neural net, many different network structures and training strategies are used in order to find the most effective one. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology and Timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the outset of this project I had a few specific goals (outside of the team goals). \n",
    "* I believed that because checkers is a fairly complex game, the neural nets would take many repetitions in order to learn. To deal with this I hoped to write neural network code that could run on the gpu to speed up the process. \n",
    "* I wanted to create a way to save and load neural nets so that if one took a long time to learn it could be used without retraining each time I opened up a notebook.\n",
    "* To make a neural net that could actually win games.\n",
    "* To find a structure that would lead to consistent training results.\n",
    "* Finally and possibly most importantly, to learn about the different parameters that go into training a neural network and see how they change the effectiveness of this strategy. \n",
    "\n",
    "\n",
    "My timeline was as follows \n",
    "* Setup simple neural net code that could run on the gpu and check to see that it worked as well as on the cpu.\n",
    "* Create a state-move vector for the network to take as input.\n",
    "* Determine the output of the neural network.\n",
    "* Implement the training based off of the code given in class that was used for towers of hanoi.\n",
    "* Create a test function and way to determine win rates in order to prove performance. \n",
    "* Create way to save and load neural nets so that I dont have to always re-train.\n",
    "* Finally, try different network structures and evaluate the performance of each and observe what led to success and failure. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of putting all my methods here up front, I will detail the methods and sources I used as this notebook tracks my progress chronologically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Neural Networks\n",
    "\n",
    "This section details my attempt to get neural network code to work on gpu. \n",
    "\n",
    "The first hurdle I had was with implementing the code on the gpu. Ive included the code I wrote as reference, as I worked on it for most of Thanksgiving break. However I was never able to get things to work correctly. I tried adapting the code from the neural network package to work on pytorch tensors. I was met with many issues involving types and passing the tensor back and forth from the gpu to the cpu. I looked into using torch networks as well, but this was as break was coming to an end and I needed to start making progress. Ultimately, I ditched GPU support and resigned myself to lots of waiting for training in the future. \n",
    "\n",
    "Code in this section requires an NVidia GPU to work, and is broken in some sections. I have included it as reference because of the time I spent on it but it is not used outside this short section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the imports I used. I generally was following the code given in class and trying to adapt it to work with my particular case. \n",
    "* `torch` is used for gpu tensors, and a quick attempt to create my own torch network.\n",
    "* `numpy` is used for its arrays and many helpful methods\n",
    "* `neuralnetworks` is the class code for creating and using neural networks. I end up using this code for the rest of the project.\n",
    "* `neuralnetworksGPUWIP` is a package that contains the additions I made to try and get torch tensors to work with the old package. I ended up not using it.\n",
    "* `math` is used for its helpful methods`\n",
    "* `matplotlib.pyplot` is used for plotting of errors and different outcomes\n",
    "* `pandas` is used to try using my old A5 code on the gpu.\n",
    "* `mlutils` is a set of utility functions for machine learning.\n",
    "* `time` is used to time and see if the code runs faster on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import neuralnetworks as nn\n",
    "import neuralnetworksGPUWIP as nnGPU\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mlutils as ml\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I try out the difference between regular numpy arrays and torch tensors pushed to the gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bd5517572eb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m## torchy tensors (default to gpu)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#converts to float for the .mean() call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mTt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m         raise RuntimeError(\n\u001b[1;32m    160\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mFound\u001b[0m \u001b[0mno\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m http://www.nvidia.com/Download/index.aspx\"\"\")\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "## Traditional arrays\n",
    "X = np.arange(10).reshape((-1,1))\n",
    "X = X.astype(float)\n",
    "T = X + 1 + np.random.uniform(-1, 1, ((10,1)))\n",
    "\n",
    "## torchy tensors (default to gpu)\n",
    "Xt = torch.from_numpy(X.astype(float)).cuda() #converts to float for the .mean() call.\n",
    "Tt = torch.from_numpy(T).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [6.]\n",
      " [7.]\n",
      " [8.]\n",
      " [9.]]\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [8.],\n",
      "        [9.]], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(Xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I initialize the two different networks. I was able to get initialization to work. My code details the changes I had to make. Most of the are adapting method that are slightly different between torch tensors and numpy arrays. For example, one has .t() as its transverse and the other has .T for transverse. Sorting out these functions allowed me to get to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpuNet = nn.NeuralNetwork(X.shape[1], 2, T.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpuNet = nnGPU.NeuralNetwork(Xt.shape[1], 2, Tt.shape[1], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually after sorting out even more method names, I was able to get the gpunet to pass training, but it was very inconsistent. This example shows how the final error is higher than the cpuNet even though they are trained under the same conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(1, [2], 1)\n",
       "   Network was trained for 101 iterations. Final error is 0.32780252256851294."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpuNet.train(X,T,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(1, [2], 1)\n",
       "   Network was trained for 101 iterations. Final error is 0.8597776191837884."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpuNet.train(Xt,Tt,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I plot the errors of each network. We see that the gpuNet is actually reducing errors, but doesn't get as low as the cpuNet does. This is one of the better examples, the cpuNet usually gets down to 0.3, whereas the gpuNet sometimes gave me results as high as 30. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6a841a29e8>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFu1JREFUeJzt3XuQXOV55/HvM9Ot6YnAFjIjVpYAyRQGHAdEMpaJ8W5sLllCXAYqJGvKSSibRE5VqIWss4md1O7GW17HVBKIs+X1rmwwSsrxJfgCRXASAjgUKQd7ZGQQFjYYYxDIaFwgLgYkzejZP/q05tY905rpntFpfT/lqZk+fXr6aZ/h16+e8/Z5IzORJJVf31IXIEnqDANdknqEgS5JPcJAl6QeYaBLUo8w0CWpRxjoktQjDHRJ6hEGuiT1iMpiPtmxxx6b69atW8ynlKTS27p1648zc2iu/RY10NetW8fIyMhiPqUklV5E/LCd/Wy5SFKPMNAlqUcY6JLUIwx0SeoRBrok9QgDXZJ6hIEuST2iFIF+x46n+T9fe2Spy5Ckw1opAv1fvjfK5rsfXeoyJOmw1nagR0R/RNwXEbcWt9dHxL0R8XBEfD4ilnWryIFKH3v3H+jWr5eknnAoI/SrgB2Tbl8DXJeZJwPPAld0srDJatV+9o6Nk5ndegpJKr22Aj0i1gK/DHyquB3AOcBNxS5bgIu7USDUR+gHEsYOGOiS1Eq7I/S/BP4AaPQ9XgPsycyx4vZOYE2zB0bEpogYiYiR0dHReRU5UOkHYO+YbRdJamXOQI+IdwC7M3Pr5M1Ndm06fM7MzZk5nJnDQ0NzXv2xqYFqvcy9+8fn9XhJOhK0c/ncs4F3RsSFQA14FfUR+4qIqBSj9LXAU90qcqBSBLojdElqac4RemZ+MDPXZuY64F3AnZn5buAu4NJit8uBm7tVpC0XSZrbQuah/yHwXyLiEeo99es7U9JMEyN0Wy6S1MohrViUmV8Dvlb8/CiwsfMlzdToob/iXHRJaqkUnxQ92HLxpKgktVSSQPekqCTNpSSB7klRSZpLOQK96klRSZpLOQK90XLxpKgktVSKQK9VbblI0lxKEejOQ5ekuZUk0B2hS9JcShHoy+yhS9KcShHo/X1BtT9suUjSLEoR6FBvu9hykaTWShTofY7QJWkWpQp0L84lSa2VJ9CrtlwkaTblCfRKn1dblKRZtLOmaC0ivhER346IByPiQ8X2GyPiBxGxrfja0M1C6z10R+iS1Eo7C1zsBc7JzBcjogrcExFfLe77r5l5U/fKm1Cf5eIIXZJaaWdN0czMF4ub1eIru1pVEwNVR+iSNJu2eugR0R8R24DdwO2ZeW9x1/+KiPsj4rqIGOhalTR66Aa6JLXSVqBn5nhmbgDWAhsj4o3AB4FTgTcBK6kvGj1DRGyKiJGIGBkdHZ13ofVZLrZcJKmVQ5rlkpl7qC8SfUFm7iraMXuBT9NiwejM3JyZw5k5PDQ0NO9CPSkqSbNrZ5bLUESsKH4eBM4DHoqI1cW2AC4GtnezUD/6L0mza2eWy2pgS0T0U38D+EJm3hoRd0bEEBDANuB3ulin89AlaQ5zBnpm3g+c2WT7OV2pqAVnuUjS7Er0SdF6yyVz0WdMSlIplCjQ66XuG3eULknNlC7QveKiJDVXnkCvNtYV9cSoJDVTnkB3XVFJmlX5At2ZLpLUVIkC3ZaLJM2mPIFedYQuSbMpTaDXGiN0e+iS1FRpAn1ihG7LRZKaKU+ge1JUkmZVokBvnBQ10CWpmRIFemMeui0XSWqmPIHuLBdJmlV5At2WiyTNqp0Vi2oR8Y2I+HZEPBgRHyq2r4+IeyPi4Yj4fEQs62ahExfnsuUiSc20M0LfC5yTmWcAG4ALIuIs4Brgusw8GXgWuKJ7ZTrLRZLmMmegFwtBv1jcrBZfCZwD3FRs30J9XdGuiQiWVfqchy5JLbTVQ4+I/ojYBuwGbge+D+zJzLFil53Amu6UOKG+rqgjdElqpq1Az8zxzNwArAU2Aqc1263ZYyNiU0SMRMTI6Ojo/CtlYhk6SdJMhzTLJTP3AF8DzgJWRERjkem1wFMtHrM5M4czc3hoaGghtdZH6LZcJKmpdma5DEXEiuLnQeA8YAdwF3BpsdvlwM3dKrKhVu1zhC5JLVTm3oXVwJaI6Kf+BvCFzLw1Ir4DfC4iPgzcB1zfxTqBouViD12Smpoz0DPzfuDMJtsfpd5PXzQDVVsuktRKaT4pCo0euiN0SWqmZIHuLBdJaqVkgd7n1RYlqYVyBXq1n32O0CWpqXIFeqXPi3NJUgulC3R76JLUXMkC3ZOiktRKuQLdeeiS1FK5Ar3Sx/7xZPxA0+uASdIRrWSBXl+GzpkukjRTyQK9sWqRbRdJmq5UgV6rulC0JLVSqkA/OEL3iouSNEO5Ar1qy0WSWilXoFdsuUhSKyULdEfoktRKO0vQHR8Rd0XEjoh4MCKuKrb/SUQ8GRHbiq8Lu12sPXRJaq2dJejGgPdn5rci4mhga0TcXtx3XWb+effKm2qgmOXyiiN0SZqhnSXodgG7ip9fiIgdwJpuF9aMI3RJau2QeugRsY76+qL3FpuujIj7I+KGiDimxWM2RcRIRIyMjo4uqNiJHrqBLknTtR3oEXEU8EXg6sx8HvgEcBKwgfoI/i+aPS4zN2fmcGYODw0NLajYgYMfLLLlIknTtRXoEVGlHuafycwvAWTm05k5npkHgE8CG7tXZp0jdElqrZ1ZLgFcD+zIzGsnbV89abdLgO2dL28qe+iS1Fo7s1zOBn4DeCAithXb/gi4LCI2AAk8BryvKxVOMvHBIlsukjRdO7Nc7gGiyV23db6c2VX7g76w5SJJzZTqk6IR4TJ0ktRCqQIdimXo9ttykaTpyhfolT5H6JLURAkD3ZaLJDVTwkDvc5aLJDVRvkCv9jkPXZKaKF+gV/q92qIkNVHCQHeELknNlDPQPSkqSTOUMND7PSkqSU2UL9CrjtAlqZnyBbo9dElqqnSBXqvacpGkZkoX6AOVPl5xhC5JM5Qu0GvV+jz0zFzqUiTpsNLOikXHR8RdEbEjIh6MiKuK7Ssj4vaIeLj43nSR6E6rVfvJhH3jjtIlabJ2RuhjwPsz8zTgLOB3I+INwAeAOzLzZOCO4nbXNZahs+0iSVPNGeiZuSszv1X8/AKwA1gDXARsKXbbAlzcrSInG1xWX4buFa+JLklTHFIPPSLWAWcC9wLHZeYuqIc+sKrTxTVTqxjoktRM24EeEUcBXwSuzsznD+FxmyJiJCJGRkdH51PjFI0R+ssGuiRN0VagR0SVeph/JjO/VGx+OiJWF/evBnY3e2xmbs7M4cwcHhoaWnDBtao9dElqpp1ZLgFcD+zIzGsn3XULcHnx8+XAzZ0vb6ZatRih73OELkmTVdrY52zgN4AHImJbse2PgI8CX4iIK4DHgV/tTolTNQLda6JL0lRzBnpm3gNEi7vP7Ww5cxtsBLojdEmaopSfFAVH6JI0XekC/eAI3ZOikjRF6QK9McvFk6KSNFUJA92WiyQ1U7pAH6j0EeFJUUmarnSBHhH1a6K7DJ0kTVG6QIf6iVF76JI0VSkDvVbt9+JckjRNKQN9sNrvxbkkaZpSBvpAtd956JI0TSkDfbDaZ8tFkqYpZaDbQ5ekmUoZ6IPVfj9YJEnTlDLQa05blKQZShvonhSVpKlKGuieFJWk6dpZgu6GiNgdEdsnbfuTiHgyIrYVXxd2t8ypBj0pKkkztDNCvxG4oMn26zJzQ/F1W2fLml2t+GBRZi7m00rSYW3OQM/Mu4FnFqGWtg0u6+dAwv5xA12SGhbSQ78yIu4vWjLHtNopIjZFxEhEjIyOji7g6SYMVIpFLmy7SNJB8w30TwAnARuAXcBftNoxMzdn5nBmDg8NDc3z6aZqLHKx10CXpIPmFeiZ+XRmjmfmAeCTwMbOljW7xrqijtAlacK8Aj0iVk+6eQmwvdW+3VBzoWhJmqEy1w4R8VngbcCxEbET+B/A2yJiA5DAY8D7uljjDIPL6u9DTl2UpAlzBnpmXtZk8/VdqKVttYotF0marpyfFF3WaLkY6JLUUM5ArxjokjRdKQN9cJknRSVpulIGeq3qB4skabpSBvpg1ZaLJE1XykCv+cEiSZqhlIHeuJaLPXRJmlDKQI8IatU+r+UiSZOUMtCh3ke35SJJE0ob6DVXLZKkKUod6C/bQ5ekg0od6I7QJWlCiQO9z0CXpElKG+iDjtAlaYrSBnrNWS6SNMWcgV4sAr07IrZP2rYyIm6PiIeL7y0Xie6W+gjdk6KS1NDOCP1G4IJp2z4A3JGZJwN3FLcX1UC1j5f3OUKXpIY5Az0z7waembb5ImBL8fMW4OIO1zWnwWo/e8cMdElqmG8P/bjM3AVQfF/VuZLaU7PlIklTdP2kaERsioiRiBgZHR3t2O/1o/+SNNV8A/3piFgNUHzf3WrHzNycmcOZOTw0NDTPp5upVu1j/ECyf9xRuiTB/AP9FuDy4ufLgZs7U077vCa6JE3VzrTFzwJfB06JiJ0RcQXwUeD8iHgYOL+4vahqrlokSVNU5tohMy9rcde5Ha7lkBxchm6fLRdJgpJ/UhTgFacuShJQ6kCvl+6HiySprrSBPmgPXZKmKG2gDzjLRZKmKG2gT4zQPSkqSVDiQG/00L2eiyTVlTbQB5cVLRdPikoSUOJAr1U8KSpJk5U20A+O0O2hSxJQ4kAfqNRLd4QuSXWlDfSIoFbtM9AlqVDaQIfGIhcGuiRByQPdRS4kaUKpA91l6CRpQqkDfaBiD12SGua8HvpsIuIx4AVgHBjLzOFOFNWuwWW2XCSpYUGBXnh7Zv64A7/nkNUq/ey15SJJQMlbLo7QJWnCQgM9gX+KiK0RsakTBR0K56FL0oSFtlzOzsynImIVcHtEPJSZd0/eoQj6TQAnnHDCAp9uqprTFiXpoAWN0DPzqeL7buDLwMYm+2zOzOHMHB4aGlrI083gtEVJmjDvQI+I5RFxdONn4BeB7Z0qrB2DflJUkg5aSMvlOODLEdH4PX+bmf/QkaraVKv28ZN9Y1z9ufsAeO2KQa4+7/Usq5T6XK8kzcu8Az0zHwXO6GAth2zj+tfw1Qd+xH1P7CETvrLtKZ59aR8fueRnKN5oJOmI0Yl56EvmF14/xJ2//7aDt//sHx/i43d9n5NXHc1737p+6QqTpCVQ6kCf7v3nn8Iju1/kw3//HdYPLeftp6xa6pIkadH0VKD39QXX/toGfvX/fp33fPqbNLouKwarXPtrG3j7qQa8pN7VU4EOsHygwo3vfRNf+OYT7BurT2m8fcdufuuvR/ifF/00737ziUtcoSR1R88FOsCqo2tcec7JB2+/7xdO4sq//RZ//OXtPP7MS/zhfzyVvj5PmkrqLUfE/L7lAxU++ZvDvPvNJ/D//uVRPvz3O8jMpS5LkjqqJ0fozVT6+/jwxW+k2t/HDf/6A46uVfi981+/1GVJUsccMYEO9YWl//s73sBP9o7xsTseZvlAP7/971/nnHVJPeGICnSoz4T56K+czkv7xvnIbQ/xv+98hDe+9tWccfwK3vvWdaw6urbUJUrSvBwRPfTp+vuC6/7TBv7s0tN55xmv5aV9Y1x/z6P84nV3c/O2J+2vSyqlWMzwGh4ezpGRkUV7vkPx/dEX+f2/+zb3Pb6H805bxbmnHcfqV9dYs2KQ9ccup9J/RL73SToMRMTWdpb4POJaLq2cNHQUN/3OW7j+nke59vbv8c87dh+8b7Daz+lrX83wumP4rbe+jmOWL1vCSiWpOUfoTewfP8DuF/aya8/LPPHsS3z7iee47/Fn2f7U87zlpNew5T0bnccuadE4Ql+Aan8fa1YMsmbFIMPrVnLJmWsB+Jt/+yH/7Svb2fL1x3jP2V78S9LhxcbwIfj1N5/Auaeu4k+/+hDf/dELS12OJE1hoB+CiOCaS0/nVbUKV33uPnY99zJ7x1wxSdLhYUEtl4i4APgY0A98KjM/2pGqDmPHHjXANb9yOldsGeHn//ROAI4aqHDKvzua4XXH8KYTV7J25SBH16ocNVBh2aTZMZM/v9QXQbU//FCTpI6Z90nRiOgHvgecD+wEvglclpnfafWYspwUbcfWHz7Dd3/0Is++tI/RF/Zy/849PPDkc+wfP7T/Pyt9MeMEa1AP/yCYnvf1+4Jo3Ji8LepvFH3Fna3eK6ZvjoO/Z+43l3Z/58T+3XnDaufXHqnvlY3jOPn1N/5GGj93uYDSaKfUTv0Nf+SSn2Hj+pXzeuxinBTdCDxSLEVHRHwOuAhoGei95OdOXMnPnTj14Lyyf5wHnnyO3c/v5cW9+3nhlTHGDtQDfvr75oFM9o0dYP/4AQ5Mui9Jiv/N+IBTHtxe7Dft9x7IJLP+/UDL95WZv7NZfc0f2XynVo9t561tPuOJVnUc8pP3oMbLnvy30/ibmXx/156/RB/Ka6vSDr6c5QP9nftlLSwk0NcAT0y6vRN488LKKbdatZ83rZvfO7AkLdRCToo2+3fIjPeziNgUESMRMTI6OrqAp5MkzWYhgb4TOH7S7bXAU9N3yszNmTmcmcNDQ0MLeDpJ0mwWEujfBE6OiPURsQx4F3BLZ8qSJB2qeffQM3MsIq4E/pH6tMUbMvPBjlUmSTokC5qHnpm3Abd1qBZJ0gL4SVFJ6hEGuiT1CANdknrEol4PPSJGgR/O8+HHAj/uYDll4Gs+MviajwwLec0nZuac874XNdAXIiJG2rmWQS/xNR8ZfM1HhsV4zbZcJKlHGOiS1CPKFOibl7qAJeBrPjL4mo8MXX/NpemhS5JmV6YRuiRpFqUI9Ii4ICK+GxGPRMQHlrqeTouI4yPirojYEREPRsRVxfaVEXF7RDxcfD9mqWvttIjoj4j7IuLW4vb6iLi3eM2fLy781jMiYkVE3BQRDxXH++d7/ThHxO8Vf9fbI+KzEVHrteMcETdExO6I2D5pW9PjGnV/VeTZ/RHxs52q47AP9GKpu48DvwS8AbgsIt6wtFV13Bjw/sw8DTgL+N3iNX4AuCMzTwbuKG73mquAHZNuXwNcV7zmZ4ErlqSq7vkY8A+ZeSpwBvXX3rPHOSLWAP8ZGM7MN1K/kN+76L3jfCNwwbRtrY7rLwEnF1+bgE90qojDPtCZtNRdZu4DGkvd9YzM3JWZ3yp+foH6f+RrqL/OLcVuW4CLl6bC7oiItcAvA58qbgdwDnBTsUtPveaIeBXwH4DrATJzX2buocePM/WLAA5GRAX4KWAXPXacM/Nu4Jlpm1sd14uAv866fwNWRMTqTtRRhkBvttTdmiWqpesiYh1wJnAvcFxm7oJ66AOrlq6yrvhL4A+AA8Xt1wB7MnOsuN1rx/p1wCjw6aLN9KmIWE4PH+fMfBL4c+Bx6kH+HLCV3j7ODa2Oa9cyrQyB3tZSd70gIo4CvghcnZnPL3U93RQR7wB2Z+bWyZub7NpLx7oC/Czwicw8E/gJPdReaaboG18ErAdeCyyn3nKYrpeO81y69ndehkBva6m7souIKvUw/0xmfqnY/HTjn2LF991LVV8XnA28MyIeo95GO4f6iH1F8U9z6L1jvRPYmZn3Frdvoh7wvXyczwN+kJmjmbkf+BLwFnr7ODe0Oq5dy7QyBHrPL3VX9I6vB3Zk5rWT7roFuLz4+XLg5sWurVsy84OZuTYz11E/pndm5ruBu4BLi9167TX/CHgiIk4pNp0LfIcePs7UWy1nRcRPFX/njdfcs8d5klbH9RbgN4vZLmcBzzVaMwuWmYf9F3Ah8D3g+8AfL3U9XXh9b6X+T677gW3F14XUe8p3AA8X31cuda1dev1vA24tfn4d8A3gEeDvgIGlrq/Dr3UDMFIc668Ax/T6cQY+BDwEbAf+BhjoteMMfJb6OYL91EfgV7Q6rtRbLh8v8uwB6jOAOlKHnxSVpB5RhpaLJKkNBrok9QgDXZJ6hIEuST3CQJekHmGgS1KPMNAlqUcY6JLUI/4/+9IyioaObF4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cpuNet.getErrors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6a84139898>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(gpuNet.getErrors())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I suspected something was wrong with my implementation, but I continued on to try out how it would do on my A5 code as I knew what sort of values to expect from that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with a5 data...\n",
    "csv = pd.read_csv(\"energydata_complete.csv\")\n",
    "#drop unneeded columns\n",
    "csv = csv.drop(['date', 'rv1', 'rv2'], axis = 1)\n",
    "data = csv.values #.values is a numpy array! \n",
    "Tenergy = data[:, :2]\n",
    "Xenergy = data[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(24, [10, 10], 2)\n",
       "   Network was trained for 10001 iterations. Final error is 4715.99911828145."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testNN = nn.NeuralNetwork(Xenergy.shape[1], [10,10], Tenergy.shape[1])\n",
    "testNN.train(Xenergy, Tenergy, 10000) #300 should be fine\n",
    "#Final error is not the same as RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(24, [10, 10], 2)\n",
       "   Network was trained for 10001 iterations. Final error is 10074.343105026634."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XenergyGPU = torch.from_numpy(Xenergy).cuda()\n",
    "TenergyGPU = torch.from_numpy(Tenergy).cuda()\n",
    "testNNGPU = nn.NeuralNetwork(XenergyGPU.shape[1], [10,10], TenergyGPU.shape[1], True)\n",
    "testNNGPU.train(XenergyGPU, TenergyGPU, 10000) #300 should be fine\n",
    "#Final error is not the same as RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you can see clearly that the error for the gpu implementation is much higher. I thought at first that it might just be an error with standardization that caused my numbers to seem higher than the results were, but the results varied wildly so went on to try a different strategy. \n",
    "\n",
    "Here I try implement this as a torch network. This was sort of a last ditch attempt to get something to work on the gpu. This definition of train is taken from the lecture code given to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, T, learning_rate_hidden, learning_rate_output, nHiddens, nIterations):\n",
    "    nSamples = X.shape[0]\n",
    "    nOutputs = T.shape[1]\n",
    "\n",
    "    # Initialize weights to uniformly distributed values between small normally-distributed between -0.1 and 0.1\n",
    "    V = 0.1*2*(np.random.uniform(size=(1+1, nHiddens))-0.5)\n",
    "    W = 0.1*2*(np.random.uniform(size=(1+nHiddens ,nOutputs))-0.5)\n",
    "\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        V = torch.from_numpy(V).cuda()\n",
    "        W = torch.from_numpy(W).cuda()\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for step in range(nIterations):\n",
    "\n",
    "        # Forward pass on training data\n",
    "        if isinstance(X, np.ndarray):\n",
    "            Z = np.tanh(X @ V[1:, :] + V[0:1, :])\n",
    "        else:\n",
    "            Z = (X @ V[1:, :] + V[0:1, :]).tanh()\n",
    "        Y = Z @ W[1:, :] + W[0:1, :]\n",
    "\n",
    "        # Error in output\n",
    "        deltaW = (T - Y) / nSamples\n",
    "\n",
    "        lrh = learning_rate_hidden\n",
    "        lro = learning_rate_output\n",
    "        # Backward pass - the backpropagation and weight update steps\n",
    "        try:\n",
    "            deltaV = (deltaW @ W[1:, :].T) * (1 - Z**2)\n",
    "            V[1:, :] += lrh * X.T @ deltaV\n",
    "            V[0:1, :] += lrh * deltaV.sum(0)\n",
    "            W[1:, :] += lro * Z.T @ deltaW\n",
    "            W[0:1, :] += lro * deltaW.sum()\n",
    "        except:\n",
    "            deltaV = (deltaW @ W[1:, :].t()) * (1 - Z**2)\n",
    "            V[1:, :] += lrh * X.t() @ deltaV\n",
    "            V[0:1, :] += lrh * deltaV.sum(0)\n",
    "            W[1:, :] += lro * Z.t() @ deltaW\n",
    "            W[0:1, :] += lro * deltaW.sum()\n",
    "           \n",
    "    print('train took', time.time() - start_time, 'seconds')\n",
    "    \n",
    "    return V, W\n",
    "def use(X, V, W):\n",
    "    if isinstance(X, np.ndarray):\n",
    "        Z = np.tanh(X @ V[1:, :] + V[0:1, :])\n",
    "    else:\n",
    "        Z = (X @ V[1:, :] + V[0:1, :]).tanh()\n",
    "    Y = Z @ W[1:, :] + W[0:1, :]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (19735,24) and (1,50) not aligned: 24 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-52c4d56201b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXenergy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTenergy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-2cc3a5e38e10>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, T, learning_rate_hidden, learning_rate_output, nHiddens, nIterations)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Forward pass on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (19735,24) and (1,50) not aligned: 24 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "V, W = train(Xenergy, Tenergy, 10, .01, 50, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pytorch(X, T, learning_rate, nHiddens, nIterations, useAdam=False):\n",
    "    \n",
    "    class Torchnn(torch.nn.Module):\n",
    "    \n",
    "        def __init__(self, n_inputs, n_hidden_units, n_outputs):\n",
    "            super(Torchnn, self).__init__()\n",
    "            self.hidden = torch.nn.Linear(n_inputs, n_hidden_units) \n",
    "            self.tanh = torch.nn.Tanh()\n",
    "            self.output = torch.nn.Linear(n_hidden_units, n_outputs)\n",
    "\n",
    "        def forward(self, X):\n",
    "            out = self.hidden(X)\n",
    "            out = self.tanh(out)\n",
    "            out = self.output(out)\n",
    "            return out\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Running on', device)\n",
    "    \n",
    "    torchnn = Torchnn(1, nHiddens, 1).to(device).double()  # default is single precision\n",
    "    \n",
    "    if useAdam:\n",
    "        optimizer = torch.optim.Adam(torchnn.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(torchnn.parameters(), lr=learning_rate)\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    \n",
    "    errors = []\n",
    "    startTime = time.time()\n",
    "\n",
    "    for iteration in range(nIterations):\n",
    "        # Forward pass\n",
    "        outputs = torchnn(X)\n",
    "        loss = loss_func(outputs, T)\n",
    "        errors.append(torch.sqrt(loss))\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Training took {} seconds'.format(time.time() - startTime))\n",
    "    \n",
    "    return torchnn, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [19735 x 24], m2: [1 x 50] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:249",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-1334a91bb593>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorchnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_pytorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXenergyGPU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTenergyGPU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-0c4a47090da5>\u001b[0m in \u001b[0;36mtrain_pytorch\u001b[0;34m(X, T, learning_rate, nHiddens, nIterations, useAdam)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnIterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-0c4a47090da5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [19735 x 24], m2: [1 x 50] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:249"
     ]
    }
   ],
   "source": [
    "torchnn, errors = train_pytorch(XenergyGPU, TenergyGPU, .01, 50, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the GPU section. As the last several code blocks show, this code is not able to handle different dimensions on input and output. I probably could have changed them to handle that, but I needed to start making progress at the end of break so I left this here. I did learn some about torch on GPUs though, so it was not a waste! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation of Board for Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section details how I created a neural network interpretable representation of the board state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the board and piece representation created earlier in the project. These objects wrap up the board in a convenient way and provide methods for modify state and getting valid moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from board import *\n",
    "from piece import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make a board with the starting state and a few checkers changed to kings to show their representation. On the top and left side of the board are the indexes of the moves to help the user view the state of the game.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0 |  1 |  2 |  3 |  4 |  5 |  6 |  7\n",
      " 0 | -  | BK | -  | B  | -  | B  | -  | B \n",
      " 1 | B  | -  | B  | -  | B  | -  | B  | - \n",
      " 2 | -  | B  | -  | B  | -  | B  | -  | B \n",
      " 3 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 4 | -  | -  | -  | -  | -  | -  | -  | - \n",
      " 5 | R  | -  | R  | -  | R  | -  | R  | - \n",
      " 6 | -  | R  | -  | R  | -  | R  | -  | R \n",
      " 7 | R  | -  | R  | -  | R  | -  | RK | - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "b = Board()\n",
    "dr = b.draught\n",
    "## add kings to show that they have *3 weight\n",
    "blackKing = Piece(Color.BLACK, (0,1))\n",
    "blackKing.king_me()\n",
    "redKing = Piece(Color.RED, (7,6))\n",
    "redKing.king_me()\n",
    "dr[0, 1] = blackKing\n",
    "dr[7, 6] = redKing\n",
    "b.draught = dr\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how almost half of the squares are empty. This is just part of the game of checkers. In the future, the board could not include these empty squares in the array to reduce space, but for this project that did not seem like a necessary optimization.\n",
    "\n",
    "Here I show the representation of the board that is passed into the neural network. For traditional reinforcement learning, a (state,move) tuple is enough to store a unique dictionary input. Howerver, for a neural net, a vector representation of the same information is needed. To achieve this, I represent the board as a list of length 64, where enemy men are -1 and enemy kings are -3 to let the network differentiate between the two. The friendly players are differentiated by 1 and 3 as well. Then, I add the move to the end as four numbers representing form , to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first of valid moves is seen easily in the board, from 0,5 to 4,1\n",
      "[((5, 0), ((4, 1),)), ((5, 2), ((4, 1),)), ((5, 2), ((4, 3),)), ((5, 4), ((4, 3),)), ((5, 4), ((4, 5),)), ((5, 6), ((4, 5),)), ((5, 6), ((4, 7),))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moves = b.validMoves()\n",
    "print(\"The first of valid moves is seen easily in the board, from 0,5 to 4,1\")\n",
    "print(moves)\n",
    "vector = b.stateMoveVectorForNN(moves[0])\n",
    "len(vector)\n",
    "#shows a vector representation of the board for the neural network.\n",
    "#see newStateRep in notebook 20(?) for more. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning With a Neural Network\n",
    "\n",
    "This section details the implementation of the nueral network. This network and training structure is based off of a combination of notebooks 15 and 21, which give code and explanaitions detailing reinforcement learning with neural nets. I took the explanation of adversarial reinforcement learning in 15 and combined it with the implementation of neural network based reinforcement learning in 21. \n",
    "\n",
    "### Structure\n",
    "Above I showed the input structure of the Neural Network to be a vector of length 68 representing the state and move. The output of the neural network will be a number from -1 to 1 representing the q value for the state and move. Selecting the best move would mean selecting the move that has the highest q value. In order to train this then, the states and values are passed in and trained to get the value of the move(found by playing games) as output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation \n",
    "\n",
    "The implementation of neural network based reinforcement learning is made in NeuralReinforcement.py. This file has the following methods:\n",
    "* `epsilonGreedy(Qnet, state, epsilon)`\n",
    "* `finished(state)`\n",
    "* `trainQnet(nBatches, nRepsPerBatch, hiddenLayers, nIterations, nReplays, epsilon, epsilonDecayFactor)`\n",
    "* `useQ(Qnet, maxSteps, printResult = False)`\n",
    "* `testQ(Qnet, trials, maxSteps = 1000, printResult = False)`\n",
    "* `plotOutcomes(outcomes, binRate = 10)`\n",
    "\n",
    "`epsilonGreedy(Qnet, state, epsilon)` takes a Qnet neural network object, the state as a Board object, and the value for epsilon as a number from 0 to 1. `epsilonGreedy` is a function that either takes a random move or the move decided by Qnet based on the value of epsilon. If a randomly generated number is less than `epsilon`, the function just returns a randomly selected move from the valid moves given by `state`. Otherwise, it decides using `Qnet`. Every valid move in the given `state` is passed in along with `state` as input the the neural network `Qnet`, then the move with the highest output is selected as the best move and returned.\n",
    "\n",
    "`finished(state)` simply takes a state and returns a boolean for whether the game is finished or not based on the number of valid moves remaining for the given player.\n",
    "\n",
    "`trainQnet(nBatches, nRepsPerBatch, hiddenLayers, nIterations, nReplays, epsilon, epsilonDecayFactor)` is the most complicated method in the file. It takes:\n",
    "* `nBatches` which represents the number of batches to train the neural network with.\n",
    "* `nRepsPerBatch` which represents the number of games to play per batch to train with.\n",
    "* `hiddenLayers` gives the structure of the neural network in the list form [#HiddenUnitsInLayer1, #HiddenUnitsInLayer2, ..., #HiddenLayerUnitsInLayern].\n",
    "* `nIterations` is the number of iterations on which to train the neural net.\n",
    "* `nReplays` is the number of replays to used for training, but I never got around to using it in the scope of this project.\n",
    "* `epsilon` gives the initial epsilon, or amount of random moves, to be used for training. \n",
    "* `epsilonDecayFactor` is the amount to decay `epsilon` by for each batch in `nBatches`\n",
    "\n",
    "The method works in the following way.\n",
    "\n",
    "* First, initialize 68 to 1 neural network with `hiddenLayers` structure as well as variables to store training information\n",
    "* For the number of batches in `nBatches`\n",
    "    * If after the first batch, decay epsilon. Create a list to hold the samples\n",
    "    * For number of repititions in `nRepititions`\n",
    "        * Take a first move given by `epsilonGreedy`\n",
    "        * While the game is not finished:\n",
    "            * Create variables for the new state and move and initialize reinforcement and Q values\n",
    "            * If the game is finished, set positive reinforcement for game.\n",
    "            * Otherwise, let black take a move and see if they win. If they do, add a negative reinforcement. \n",
    "            * At the end, add the game and reinforcement to samples\n",
    "            * Then, take the `epsilonGreedy` move and move new state and moves to the old ones. \n",
    "    * Train the neural net on the information gathered after the previous repititions. \n",
    "* Return Qnet\n",
    "\n",
    "* `useQ(Qnet, maxSteps, printResult = False)` plays a game using the greedy choices based on Qnet against a random opponent with `maxSteps` as a cutoff. If printResult is True, it also returns a list of states. Otherwise it returns 0 or 1 for a loss or a win. \n",
    "* `testQ(Qnet, trials, maxSteps = 1000, printResult = False)` calls `useQ` for number of trails to roughly estimate the performance of Qnet against a random opponent.\n",
    "* `plotOutcomes(outcomes, binRate = 10)` plots the outcomes of training a neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using and Testing The Neural Net\n",
    "The following section details my procedure for testing different network structures and my evaluation of their success. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import several packages that are needed. `NeuralReinforcement` contains the implementaion. nnutils is a utility file that contains helper methods described bellow. The rest of the imports are utilities to timing or other small methods in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NeuralReinforcement as nr\n",
    "import nnutils \n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from board import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nnutils` contains some helping functions. \n",
    "\n",
    "* `loadNetwork(filename)` reads in a Qnet named \"filename\" from the disk and returns it as an object. Reads from directory qnets/.\n",
    "* `saveNetwork(filename, Qnet)` saves a Qnet object as \"filename\" in directory qnets/\n",
    "* `trainNN(nBatches, nRepsPerbatch, hiddenLayers, nIterations, epsilonDecayFactor)` trains and times a neural net given the specified parameters. \n",
    "* `randomQ(maxSteps)` simulates a game of two random players against each other. Used to compare our strategies to a \"stupid\" strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tested the neural nets itermitently over a few weeks. I was not getting great results at first, starting with win rates at 55%, and eventually getting my best at 70%. But during one of the presentations in the last week, someone had a huge network structure, so I decided to try that. That plus fewer iterations improved performance more, eventually getting up to an 87% win rate! I also found that very few batches with a lot of reps was more favorable. My best win rate was with a 1000 hidden unit network trained on 1000 games that won around 99% of games against a random opponent!\n",
    "\n",
    "Here I will show the the performance of my first structures and some simple structures. Then show the better ones. All of the Qnets will be saved using `dill` and put in a the director `qnets/` so they can be used without retraining. Later in the notebook I will discuss why I chose the different structures and parameters, but for now these examples just illustrate trails from different points of my progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    qnet68 = nnutils.loadNetwork(\"68.qnet\")\n",
    "except:\n",
    "    qnet68 = nnutils.trainNN(20, 10, [68], 10, .97)\n",
    "    saveNetwork(\"68.qnet\", qnet68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red won 62.40% of games!\n"
     ]
    }
   ],
   "source": [
    "res = nr.testQ(qnet68, 1000, maxSteps = 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    qnet50_50_5020x10 = nnutils.loadNetwork(\"50_50_5020x10.qnet\")\n",
    "except:\n",
    "    qnet50_50_5020x10 = nnutils.trainNN(20, 10, [50, 50, 50], 10, .97)\n",
    "    saveNetwork(\"50_50_5020x10.qnet\", qnet50_50_5020x10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red won 57.40% of games!\n"
     ]
    }
   ],
   "source": [
    "res = nr.testQ(qnet50_50_5020x10, 1000, maxSteps = 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    qnet50_50_50 = nnutils.loadNetwork(\"50_50_50.qnet\")\n",
    "except:\n",
    "    qnet50_50_50 = nnutils.trainNN(2, 100, [50, 50, 50], 10, .97)\n",
    "    saveNetwork(\"50_50_50.qnet\", qnet50_50_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red won 65.80% of games!\n"
     ]
    }
   ],
   "source": [
    "res = nr.testQ(qnet50_50_50, 1000, maxSteps = 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes around two minutes to train and gets a respectable win rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    qnet100_100 = nnutils.loadNetwork(\"100_100.qnet\")\n",
    "except:\n",
    "    qnet100_100 = nnutils.trainNN(2, 100, [100,100], 10, .97)\n",
    "    saveNetwork(\"100_100.qnet\", qnet100_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red won 82.10% of games!\n"
     ]
    }
   ],
   "source": [
    "res = nr.testQ(qnet100_100, 1000, maxSteps = 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the go big or go home network, trained on 1000 games with 1000 hidden units. It trained for five hours on my 8 core cpu, so I'm glad these networks can be saved! Ended up with strong results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    qnet1000 = nnutils.loadNetwork(\"1000.qnet\")\n",
    "except:\n",
    "    qnet1000 = nnutils.trainNN(2, 500, [1000], 10, .97)\n",
    "    saveNetwork(\"1000.qnet\", qnet1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red won 98.20% of games!\n"
     ]
    }
   ],
   "source": [
    "res = nr.testQ(qnet1000, 1000, maxSteps = 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like 5 hours of training was worth it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I plot the outcomes of one of the trained neural nets. As I will explain later though, there were not enough batches to have epsilon greedy start choosing good moves, so the graph does not make much progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nnutils.plotOutcomes(qnet50_50_5020x10.outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I demonstrate what the q values look like from the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a demonstration board\n",
    "b = Board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves = b.validMoves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we print out all of the Q values for the starting state of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02078446]]\n",
      "[[0.02232421]]\n",
      "[[0.0392772]]\n",
      "[[0.04072942]]\n",
      "[[0.03672907]]\n",
      "[[0.03459328]]\n",
      "[[0.01826239]]\n"
     ]
    }
   ],
   "source": [
    "for move in moves:\n",
    "    X = b.stateMoveVectorForNN(move)\n",
    "    # without this, we give it a list. Needs an np array.\n",
    "    X = np.array(X)\n",
    "    # expects a 2d array. We want one row of a that array, so reshape first.\n",
    "    X = X.reshape(1, 68)\n",
    "    print(qnet1000.use(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((5, 4), ((4, 3),)), 0.04072942055390925)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr.epsilonGreedy(qnet1000, b, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the results of the 1000 unit neural network. While the reinforcement values are fairly sparse, output can be found for every state of the game. Next we verify that there isn't a bias in checkers that leads red to win to see if the 99% win rate is notable. We play random moves against each other to check to see that red wins around 50% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_wins = []\n",
    "for _ in range(100):\n",
    "    random_wins.append(nnutils.randomQ(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(random_wins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above trial shows that random players against each other do in fact win against each other at a rate of 50%. This means that win rates significantly above 50% indicate that the network has learned some sort of strategy (although as noted elsewhere, simply picking the first move in valid_moves can lead to win rates as high as 98%). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of Neural Network\n",
    "The samples above show only some of the structures I tried. The very fist call I made was with this (30, 100, [50, 50, 50], 20, 0, 1, .99) call. This produced a valid neural network which won a measly 55.85% of the time after an hour and a half of training. But because I had done trails with `randomQ` I knew that the network had learned something. I looked through the code and found a few bugs that would improve my run time, including a bug that meant I was accidentally training for both black and red players. But even with that and different network structures, the networks could not do much better than a 60% win rate when called with the same other parameters. What I eventually found was that tweaking the number of iterations, batches and reps could actually make a large difference in effectiveness. Very low batch and iteration counts, like 2 and 10 respectively in conjuction with high rep counts in the hundreds of games gave much more effective training. This is counter intuitive, as it almost negates the role of epsilon decay factor as the first few games are essentially random moves. However, this strategy resulted in the most effective results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations and Setbacks\n",
    "\n",
    "The first setback I had was with getting some sort of code to run on the GPU. I had figured it would be a simple matter of changing a few lines of code to work with pytorch tensors, but I found it to be more complex than that. Looking back, I should have tried a few simple cases with writing my own neural net code in pytorch to more fully grasp the difference between that and the class neural network code. \n",
    "\n",
    "The next observation I had was with implementing the code. What was difficult here was determining what I was actually training the neural network on and what values it needed to target. I found that training was more successful with very few batches, most likely because of the number of iterations used to train the neural network. \n",
    "\n",
    "The biggest setback I had was probably due to time. Training these networks takes awhile on the GPU, so trying out many different large structures was not easy or feasible to do. In the end I tried a few smaller structures and guessed about what my be an effective large structure to see how it would tun. In the end it seemed to be fairly effective, winning a large portion of games consistently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for file Williams-Newell-Haynes-Final-Project.ipynb is 8262\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from nbformat import current\n",
    "import glob\n",
    "nbfile = glob.glob('Williams-Newell-Haynes-Final-Project.ipynb')\n",
    "if len(nbfile) > 1:\n",
    "    print('More than one ipynb file. Using the first one.  nbfile=', nbfile)\n",
    "with io.open(nbfile[0], 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "word_count = 0\n",
    "for cell in nb.worksheets[0].cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print('Word count for file', nbfile[0], 'is', word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
